# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


trigger:
  branches:
    include:
    - '*' 

resources:
  containers:
  # Container with Maven 3.2.5 to have the same environment everywhere.
  - container: flink-build-container
    image: rmetzger/flink-ci:4-nouser

variables:
  MAVEN_CACHE_FOLDER: $(Pipeline.Workspace)/.m2/repository
  MAVEN_OPTS: '-Dmaven.repo.local=$(MAVEN_CACHE_FOLDER)'
  CACHE_KEY: maven | maven-mirror-google | $(Agent.OS) | **/pom.xml
  CACHE_FALLBACK_KEY: maven | $(Agent.OS)
  CACHE_FLINK_DIR: $(Pipeline.Workspace)/flink_cache
  MODE: tests


jobs:
# - job: runOnDefaultAgentPool
#   pool:
#     name: Default
#   container: flink-build-container
#   timeoutInMinutes: 120
#   steps:
#   # Azure pipelines can only evaluate conditions with the build repo name in the steps.
#   # if the repo != "flink-ci/flink", we stop
#   - script: echo $(Pipeline.Workspace)
#   - script: pwd
#   - script: exit 1
#     condition: not(eq(variables['Build.Repository.Name'], 'rmetzger/flink'))
#   - script: sudo chown -R user:user /home/user
#   - script: sudo chown -R user:user .
#   - script: sudo mkdir -p $(MAVEN_CACHE_FOLDER)
#   - script: sudo chown -R user:user $(MAVEN_CACHE_FOLDER)
#   - script: ls -lisah /home/user/.m2/repository
#   - task: CacheBeta@1
#     inputs:
#       key: $(CACHE_KEY)
#       restoreKeys: $(CACHE_FALLBACK_KEY)
#       path: $(MAVEN_CACHE_FOLDER)
#       cacheHitVar: CACHE_RESTORED
#     displayName: Cache Maven local repo
#   - script: sudo chown -R user:user $(MAVEN_CACHE_FOLDER)
#   - script: ls -lisah $(MAVEN_CACHE_FOLDER)
# #  - script: mvn clean install
# #    displayName: Build
#   - task: Maven@3
#     inputs:
#       mavenPomFile: 'pom.xml'
#       mavenOptions: '-Xmx3072m'
#       javaHomeOption: 'JDKVersion'
#       jdkVersionOption: '1.8'
#       jdkArchitectureOption: 'x64'
#       publishJUnitResults: true
#       testResultsFiles: '**/surefire-reports/TEST-*.xml'
#       goals: 'clean install'
#       timeoutInMinutes: 120


- job: runOnAzure_e2e
  pool:
    vmImage: 'ubuntu-latest'
  container: flink-build-container
  timeoutInMinutes: 240
  steps:
    - script: echo "Wrong repository or mode"; exit 1
      condition: and(not(eq(variables['Build.Repository.Name'], 'rmetzger/flink')),not(eq(variables['mode'], 'e2e')))
    - task: CacheBeta@1
    inputs:
      key: $(CACHE_KEY)
      restoreKeys: $(CACHE_FALLBACK_KEY)
      path: $(MAVEN_CACHE_FOLDER)
      cacheHitVar: CACHE_RESTORED
    displayName: Cache Maven local repo
    - script: STAGE=compile ./tools/azure_controller.sh compile
    displayName: Build
    - script: FLINK_DIR=`pwd`/build-target flink-end-to-end-tests/run-nightly-tests.sh



- job: runOnAzure_compile
  pool:
    vmImage: 'ubuntu-latest'
  container: flink-build-container
  timeoutInMinutes: 240
  steps:

  # Azure pipelines can only evaluate conditions with the build repo name in the steps.
  # if the repo == "flink-ci/flink", we stop

  # Preparation
  - script: echo "Wrong repository or mode"; exit 1
    condition: and(not(eq(variables['Build.Repository.Name'], 'rmetzger/flink')),eq(variables['mode'], 'e2e'))
  #- script: sudo chown -R user:user /home/user
  #- script: sudo chown -R user:user .
  #- script: sudo mkdir -p $(MAVEN_CACHE_FOLDER)
  #- script: sudo chown -R user:user $(MAVEN_CACHE_FOLDER)
  #- script: sudo chmod -R 777 /home/user
  #- script: sudo chown -R user:azure_pipelines_docker /home/user
  #- script: ls -lisah $(MAVEN_CACHE_FOLDER) ; find $(MAVEN_CACHE_FOLDER)
  #- script: pwd ; ls -lisah . ; ls -lisah $(Pipeline.Workspace) 
  # user vsts_azpcontainer is in group azure_pipelines_docker?
  # Idea: make user part of the azure_pipelines_docker group. allow this group everything
  #- script: groups vsts_azpcontainer
  - task: CacheBeta@1
    inputs:
      key: $(CACHE_KEY)
      restoreKeys: $(CACHE_FALLBACK_KEY)
      path: $(MAVEN_CACHE_FOLDER)
      cacheHitVar: CACHE_RESTORED
    displayName: Cache Maven local repo
  #- script: sudo chown -R user:user $(MAVEN_CACHE_FOLDER)

  # Compile
  # backup: STAGE=compile HOME=/home/user sudo --preserve-env su --preserve-environment user
  - script: STAGE=compile ./tools/azure_controller.sh compile
    displayName: Build

  # upload artifacts for next stage
  - task: PublishPipelineArtifact@1
    inputs:
      path: $(CACHE_FLINK_DIR)
      artifact: FlinkCompileCacheDir

- job: runOnAzure_test
  dependsOn: runOnAzure_compile
  pool:
    vmImage: 'ubuntu-latest'
  container: flink-build-container
  timeoutInMinutes: 240
  strategy:
    parallel: 10
  steps:

  # Preparation
  - script: exit 1
    condition: not(eq(variables['Build.Repository.Name'], 'rmetzger/flink'))

  # download artifacts
  - task: DownloadPipelineArtifact@2
    inputs:
      path: $(CACHE_FLINK_DIR)
      artifact: FlinkCompileCacheDir

  # Test
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh core
    displayName: Test - core
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh python
    displayName: Test - python
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh libraries
    displayName: Test - libraries
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh blink_planner
    displayName: Test - blink_planner
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh connectors
    displayName: Test - connectors

  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh kafka/gelly
    displayName: Test - kafka/gelly
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh tests
    displayName: Test - tests
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh scheduler_ng_core
    displayName: Test - scheduler_ng_core
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh scheduler_ng_tests
    displayName: Test - scheduler_ng_tests
  - script: STAGE=test PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11" ./tools/azure_controller.sh misc
    displayName: Test - misc

    

