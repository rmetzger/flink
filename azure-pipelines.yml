# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


trigger:
  branches:
    include:
    - '*' 

schedules:
- cron: "0 0 * * *"
  displayName: Daily midnight build
  branches:
    include:
    - master
    - azure_playground
  always: true # run even if there were no changes to the mentioned branches

resources:
  containers:
  # Container with Maven 3.2.5, SSL to have the same environment everywhere.
  - container: flink-build-container
    image: rmetzger/flink-ci:5-ubuntu-amd64

variables:
  MAVEN_CACHE_FOLDER: $(Pipeline.Workspace)/.m2/repository
  MAVEN_OPTS: '-Dmaven.repo.local=$(MAVEN_CACHE_FOLDER)'
  CACHE_KEY: maven | $(Agent.OS) | **/pom.xml
  CACHE_FALLBACK_KEY: maven | $(Agent.OS)
  CACHE_FLINK_DIR: $(Pipeline.Workspace)/flink_cache

stages:
  # CI / PR triggered stage:
  - stage: ci_build_on_default_pool
    displayName: "Build on 'Default' pool (Flink Community Build Resources)"
    condition: not(eq(variables['Build.Reason'], 'Schedule'))
    jobs:
      - template: tools/azure-pipelines/azure-jobs.yml
        parameters:
          stage_name: ci_build_on_default_pool
          pool_definition:
            name: Default
          test_strategy:
              matrix:
                core:
                  module: core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                python:
                  module: python
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                libraries:
                  module: libraries
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                blink_planner:
                  module: blink_planner
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                connectors:
                  module: connectors
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                kafka_gelly:
                  module: kafka/gelly
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                tests:
                  module: tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                scheduler_ng_core:
                  module: scheduler_ng_core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                scheduler_ng_tests:
                  module: scheduler_ng_tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                misc:
                  module: misc
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"

  # CI / PR triggered stage:          
  - stage: ci_build_on_azure_os_free_pool
    displayName: "Build on free (for OS) Azure Resource Pool"
    dependsOn: [] # depending on an empty array makes the stages run in parallel
    condition: not(eq(variables['Build.Reason'], 'Schedule'))
    jobs:
      - template: tools/azure-pipelines/azure-jobs.yml
        parameters:
          stage_name: ci_build_on_azure_os_free_pool
          pool_definition:
            vmImage: 'ubuntu-latest'
          test_strategy:
              matrix:
                core:
                  module: core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                python:
                  module: python
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                libraries:
                  module: libraries
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                blink_planner:
                  module: blink_planner
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                connectors:
                  module: connectors
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                kafka_gelly:
                  module: kafka/gelly
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                tests:
                  module: tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                scheduler_ng_core:
                  module: scheduler_ng_core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                scheduler_ng_tests:
                  module: scheduler_ng_tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                misc:
                  module: misc
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"

  # Special stage for midnight build:
  - stage: cron_build_on_azure_os_free_pool
    displayName: "Cron build on free (for OS) Azure Resource Pool"
    dependsOn: [] # depending on an empty array makes the stages run in parallel
    condition: eq(variables['Build.Reason'], 'Schedule')
    jobs:
      - template: tools/azure-pipelines/azure-jobs.yml
        parameters:
          stage_name: cron_build_on_azure_os_free_pool
          pool_definition:
            vmImage: 'ubuntu-latest'
          test_strategy:
              matrix:
                # regular build
                core:
                  module: core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                python:
                  module: python
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                libraries:
                  module: libraries
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                blink_planner:
                  module: blink_planner
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                connectors:
                  module: connectors
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                kafka_gelly:
                  module: kafka/gelly
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                tests:
                  module: tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                scheduler_ng_core:
                  module: scheduler_ng_core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                scheduler_ng_tests:
                  module: scheduler_ng_tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                misc:
                  module: misc
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
                # scala 2.12
                core212:
                  module: core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                python212:
                  module: python
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                libraries212:
                  module: libraries
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                blink_planner212:
                  module: blink_planner
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                connectors212:
                  module: connectors
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                kafka_gelly212:
                  module: kafka/gelly
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                tests212:
                  module: tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                scheduler_ng_core212:
                  module: scheduler_ng_core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                scheduler_ng_tests212:
                  module: scheduler_ng_tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                misc212:
                  module: misc
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1"
                # hadoop 241
                core_hadoop241:
                  module: core
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                python_hadoop241:
                  module: python
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                libraries_hadoop241:
                  module: libraries
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                blink_planner_hadoop241:
                  module: blink_planner
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                connectors_hadoop241:
                  module: connectors
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                kafka_gelly_hadoop241:
                  module: kafka/gelly
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                tests_hadoop241:
                  module: tests
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                scheduler_ng_core_hadoop241:
                  module: scheduler_ng_core
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                scheduler_ng_tests_hadoop241:
                  module: scheduler_ng_tests
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                misc_hadoop241:
                  module: misc
                  environment: PROFILE="-Dhadoop.version=2.4.1 -Pskip-hive-tests"
                # jdk11
                core_jdk11:
                  module: core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                python_jdk11:
                  module: python
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                libraries_jdk11:
                  module: libraries
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                blink_planner_jdk11:
                  module: blink_planner
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                connectors_jdk11:
                  module: connectors
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                kafka_gelly_jdk11:
                  module: kafka/gelly
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                tests_jdk11:
                  module: tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                scheduler_ng_core_jdk11:
                  module: scheduler_ng_core
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                scheduler_ng_tests_jdk11:
                  module: scheduler_ng_tests
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"
                misc_jdk11:
                  module: misc
                  environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -Djdk11"


